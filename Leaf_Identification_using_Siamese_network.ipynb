{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/ovslW3qb8qbm5xRO69Xt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkitAnandMastery17/Blog/blob/main/Leaf_Identification_using_Siamese_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "import keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRWtW9uw5J9z",
        "outputId": "c9ce0e43-66f3-4f02-8f7d-560434a09029"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oyVQD-u_4YPx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import Model\n",
        "from keras.preprocessing import image\n",
        "from PIL import Image\n",
        "from keras.layers import Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import resnet\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, GlobalAveragePooling2D, Dense, Dropout\n",
        "from keras.layers import concatenate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgBxJJcl6Sks",
        "outputId": "6ae472fd-603b-414a-a9b8-15671eea593a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opendatasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkasgnl06ZZl",
        "outputId": "0a13e0ba-38bc-4ec1-a46e-6c8b191cebb4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.8/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from opendatasets) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.25.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (8.0.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (4.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY8IhciS6dwL",
        "outputId": "c5fd1579-fea8-4567-fc4d-5ca68040532d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "  \n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/christofel04/tomatoes-diseases-dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgbSxlET6nD2",
        "outputId": "4fc5ef9d-1d33-42fb-a6b1-1b380d786a18"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./tomatoes-diseases-dataset\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = r'/content/tomatoes-diseases-dataset/Tomatoes Diseases Dataset/Dataset Train'\n",
        "train_test_split = 0.7\n",
        "no_of_files_in_each_class = 10\n",
        "\n",
        "#Read all the folders in the directory\n",
        "folder_list = os.listdir(base_dir)\n",
        "print( len(folder_list), \"categories found in the dataset\")\n",
        "\n",
        "#Declare training array\n",
        "cat_list = []\n",
        "x = []\n",
        "y = []\n",
        "y_label = 0\n",
        "\n",
        "#Using just 5 images per category\n",
        "for folder_name in folder_list:\n",
        "    files_list = os.listdir(os.path.join(base_dir, folder_name))\n",
        "    temp=[]\n",
        "    for file_name in files_list[:no_of_files_in_each_class]:\n",
        "        temp.append(len(x))\n",
        "        x.append(np.asarray(Image.open(os.path.join(base_dir, folder_name, file_name)).convert('RGB').resize((100, 100))))\n",
        "        y.append(y_label)\n",
        "    y_label+=1\n",
        "    cat_list.append(temp)\n",
        "\n",
        "cat_list = np.asarray(cat_list)\n",
        "x = np.asarray(x)/255.0\n",
        "y = np.asarray(y)\n",
        "print('X, Y shape',x.shape, y.shape, cat_list.shape)       "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8phYux9BG23",
        "outputId": "1d2a55f3-6dd1-4c25-b856-9781f830a778"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 categories found in the dataset\n",
            "X, Y shape (100, 100, 100, 3) (100,) (10, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(folder_list)*train_test_split)\n",
        "test_size = len(folder_list) - train_size\n",
        "print(train_size, 'classes for training and', test_size, ' classes for testing')\n",
        "\n",
        "train_files = train_size * no_of_files_in_each_class\n",
        "\n",
        "#Training Split\n",
        "x_train = x[:train_files]\n",
        "y_train = y[:train_files]\n",
        "cat_train = cat_list[:train_size]\n",
        "\n",
        "#Validation Split\n",
        "x_val = x[train_files:]\n",
        "y_val = y[train_files:]\n",
        "cat_test = cat_list[train_size:]\n",
        "\n",
        "print('X&Y shape of training data :',x_train.shape, 'and', y_train.shape, cat_train.shape)\n",
        "print('X&Y shape of testing data :' , x_val.shape, 'and', y_val.shape, cat_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQj3p384CWFH",
        "outputId": "9dd9c28d-df64-4ca6-c03d-7816874434dc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 classes for training and 3  classes for testing\n",
            "X&Y shape of training data : (70, 100, 100, 3) and (70,) (7, 10)\n",
            "X&Y shape of testing data : (30, 100, 100, 3) and (30,) (3, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(batch_size=64):\n",
        "    \n",
        "    temp_x = x_train\n",
        "    temp_cat_list = cat_train\n",
        "    start=0\n",
        "    end=train_size\n",
        "    batch_x=[]\n",
        "        \n",
        "    batch_y = np.zeros(batch_size)\n",
        "    batch_y[int(batch_size/2):] = 1\n",
        "    np.random.shuffle(batch_y)\n",
        "    \n",
        "    class_list = np.random.randint(start, end, batch_size) \n",
        "    batch_x.append(np.zeros((batch_size, 112, 112, 3)))\n",
        "    batch_x.append(np.zeros((batch_size, 112, 112, 3)))\n",
        "\n",
        "    for i in range(0, batch_size):\n",
        "        batch_x[0][i] = temp_x[np.random.choice(temp_cat_list[class_list[i]])]  \n",
        "        #If train_y has 0 pick from the same class, else pick from any other class\n",
        "        if batch_y[i]==0:\n",
        "            batch_x[1][i] = temp_x[np.random.choice(temp_cat_list[class_list[i]])]\n",
        "\n",
        "        else:\n",
        "            temp_list = np.append(temp_cat_list[:class_list[i]].flatten(), temp_cat_list[class_list[i]+1:].flatten())\n",
        "            batch_x[1][i] = temp_x[np.random.choice(temp_list)]\n",
        "            \n",
        "    return(batch_x, batch_y)"
      ],
      "metadata": {
        "id": "tfxd16jMClff"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    def Inception_block1(input_layer, f1, f2_conv1, f2_conv3, f3_conv1, f3_conv5, f4): \n",
        "  # Input: \n",
        "  # - f1: number of filters of the 1x1 convolutional layer in the first path\n",
        "  # - f2_conv1, f2_conv3 are number of filters corresponding to the 1x1 and 3x3 convolutional layers in the second path\n",
        "  # - f3_conv1, f3_conv5 are the number of filters corresponding to the 1x1 and 5x5  convolutional layer in the third path\n",
        "  # - f4: number of filters of the 1x1 convolutional layer in the fourth path\n",
        "\n",
        "  # 1st path:\n",
        "  path1 = Conv2D(filters=f1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "\n",
        "  # 2nd path\n",
        "  path2 = Conv2D(filters = f2_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "  path2 = Conv2D(filters = f2_conv3, kernel_size = (5,5), padding = 'same', activation = 'relu')(path2)\n",
        "\n",
        "  # 3rd path\n",
        "  path3 = Conv2D(filters = f3_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "  path3 = Conv2D(filters = f3_conv5, kernel_size = (3,3), padding = 'same', activation = 'relu')(path3)\n",
        "  path3 = Conv2D(filters = f3_conv5, kernel_size = (3,3), padding = 'same', activation = 'relu')(path3)\n",
        "\n",
        "\n",
        "  # 4th path\n",
        "  path4 = AveragePooling2D((3,3), strides= (1,1), padding = 'same')(input_layer)\n",
        "  path4 = Conv2D(filters = f4, kernel_size = (1,1), padding = 'same', activation = 'relu')(path4)\n",
        "\n",
        "  output_layer = concatenate([path1, path2, path3, path4], axis = -1)\n",
        "\n",
        "  return output_layer"
      ],
      "metadata": {
        "id": "7PqDGKAlfg5x"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def Inception_block2(input_layer, f1, f2_conv1, f2_conv3, f3_conv1, f3_conv5, f4): \n",
        "  # Input: \n",
        "  # - f1: number of filters of the 1x1 convolutional layer in the first path\n",
        "  # - f2_conv1, f2_conv3 are number of filters corresponding to the 1x1 and 3x3 convolutional layers in the second path\n",
        "  # - f3_conv1, f3_conv5 are the number of filters corresponding to the 1x1 and 5x5  convolutional layer in the third path\n",
        "  # - f4: number of filters of the 1x1 convolutional layer in the fourth path\n",
        "\n",
        "  # 1st path:\n",
        "  path1 = Conv2D(filters=f1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "\n",
        "  # 2nd path\n",
        "  path2 = Conv2D(filters = f2_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "  path2 = Conv2D(filters = f2_conv3, kernel_size = (5,5), padding = 'same', activation = 'relu')(path2)\n",
        "\n",
        "  # 3rd path\n",
        " # path3 = Conv2D(filters = f3_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "  #path3 = Conv2D(filters = f3_conv5, kernel_size = (3,3), padding = 'same', activation = 'relu')(path3)\n",
        "   #path3 = Conv2D(filters = f3_conv5, kernel_size = (3,3), padding = 'same', activation = 'relu')(path3)\n",
        "\n",
        "\n",
        "  # 4th path\n",
        "  path4 = MaxPooling2D((3,3), strides= (1,1), padding = 'same')(input_layer)\n",
        "  path4 = Conv2D(filters = f4, kernel_size = (1,1), padding = 'same', activation = 'relu')(path4)\n",
        "\n",
        "  output_layer = concatenate([path1, path2, path3, path4], axis = -1)\n",
        "\n",
        "  return output_layer"
      ],
      "metadata": {
        "id": "xsF0FUdUof9R"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def Inception_block3(input_layer, f1, f2_conv1, f2_conv3, f3_conv1, f3_conv5, f4): \n",
        "  # Input: \n",
        "  # - f1: number of filters of the 1x1 convolutional layer in the first path\n",
        "  # - f2_conv1, f2_conv3 are number of filters corresponding to the 1x1 and 3x3 convolutional layers in the second path\n",
        "  # - f3_conv1, f3_conv5 are the number of filters corresponding to the 1x1 and 5x5  convolutional layer in the third path\n",
        "  # - f4: number of filters of the 1x1 convolutional layer in the fourth path\n",
        "\n",
        "  # 1st path:\n",
        "  path1 = Conv2D(filters=f1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "\n",
        "  # 2nd path\n",
        "  path2 = Conv2D(filters = f2_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "  path2 = Conv2D(filters = f2_conv3, kernel_size = (1,5), padding = 'same', activation = 'relu')(path2)\n",
        "  path2 = Conv2D(filters = f2_conv3, kernel_size = (5,1), padding = 'same', activation = 'relu')(path2)\n",
        "\n",
        "  # 3rd path\n",
        "  path3 = Conv2D(filters = f3_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "  path3 = Conv2D(filters = f3_conv5, kernel_size = (1,3), padding = 'same', activation = 'relu')(path3)\n",
        "  path3 = Conv2D(filters = f3_conv5, kernel_size = (3,1), padding = 'same', activation = 'relu')(path3)\n",
        "\n",
        "\n",
        "  # 4th path\n",
        "  path4 = AveragePooling2D((3,3), strides= (1,1), padding = 'same')(input_layer)\n",
        "  path4 = Conv2D(filters = f4, kernel_size = (1,1), padding = 'same', activation = 'relu')(path4)\n",
        "\n",
        "  output_layer = concatenate([path1, path2, path3, path4], axis = -1)\n",
        "\n",
        "  return output_layer"
      ],
      "metadata": {
        "id": "WQEFlOpmwOb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Inception_block2(input_layer, f1, f2_conv1, f2_conv3, f3_conv1, f3_conv5, f4): \n",
        "  # Input: \n",
        "  # - f1: number of filters of the 1x1 convolutional layer in the first path\n",
        "  # - f2_conv1, f2_conv3 are number of filters corresponding to the 1x1 and 3x3 convolutional layers in the second path\n",
        "  # - f3_conv1, f3_conv5 are the number of filters corresponding to the 1x1 and 5x5  convolutional layer in the third path\n",
        "  # - f4: number of filters of the 1x1 convolutional layer in the fourth path\n",
        "\n",
        "  # 1st path:\n",
        "  path1 = Conv2D(filters=f1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "\n",
        "  # 2nd path\n",
        "  path2 = Conv2D(filters = f2_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "  path2 = Conv2D(filters = f2_conv3, kernel_size = (1,5), padding = 'same', activation = 'relu')(path2)\n",
        "   path2 = Conv2D(filters = f2_conv3, kernel_size = (5,1), padding = 'same', activation = 'relu')(path2)\n",
        "\n",
        "  # 3rd path\n",
        " # path3 = Conv2D(filters = f3_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
        "  #path3 = Conv2D(filters = f3_conv5, kernel_size = (3,3), padding = 'same', activation = 'relu')(path3)\n",
        "   #path3 = Conv2D(filters = f3_conv5, kernel_size = (3,3), padding = 'same', activation = 'relu')(path3)\n",
        "\n",
        "\n",
        "  # 4th path\n",
        "  path4 = AveragePooling2D((3,3), strides= (1,1), padding = 'same')(input_layer)\n",
        "  path4 = Conv2D(filters = f4, kernel_size = (1,1), padding = 'same', activation = 'relu')(path4)\n",
        "\n",
        "  output_layer = concatenate([path1, path2, path3, path4], axis = -1)"
      ],
      "metadata": {
        "id": "n-rCqKity2jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GoogLeNet():\n",
        "  # input layer \n",
        "  input_layer = Input(shape = (100, 100, 3))\n",
        "\n",
        "  X = Conv2D(filters = 32, kernel_size = (3,3), padding = 'valid', activation = 'relu')(input_layer)\n",
        "\n",
        "  X = Conv2D(filters = 32, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu')(X)\n",
        "\n",
        "  X = Conv2D(filters = 64, kernel_size = (3,3),strides = 1, padding = 'same', activation = 'relu')(X)\n",
        "  X = Conv2D(filters = 64, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu')(X)\n",
        "  X = Conv2D(filters = 80, kernel_size = (3,3),strides = 1, padding = 'same', activation = 'relu')(X)\n",
        "  X = Conv2D(filters = 80, kernel_size = (3,3),strides = 1, padding = 'same', activation = 'relu')(X)\n",
        "  X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n",
        " \n",
        "\n",
        "  # 1st Inception block\n",
        "  X = Inception_block(X, f1 = 64, f2_conv1 = 96, f2_conv3 = 128, f3_conv1 = 16, f3_conv5 = 32, f4 = 32)\n",
        "\n",
        "  # 2nd Inception block\n",
        "  X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 192, f3_conv1 = 32, f3_conv5 = 96, f4 = 64)\n",
        "\n",
        "  # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "  X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n",
        "\n",
        "   # 1st Inception block\n",
        "  X = Inception_block(X, f1 = 64, f2_conv1 = 96, f2_conv3 = 128, f3_conv1 = 16, f3_conv5 = 32, f4 = 32)\n",
        "\n",
        "  # 2nd Inception block\n",
        "  X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 192, f3_conv1 = 32, f3_conv5 = 96, f4 = 64)\n",
        "\n",
        "  # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "\n",
        "\n",
        "  # 3rd Inception block\n",
        "  X = Inception_block(X, f1 = 192, f2_conv1 = 96, f2_conv3 = 208, f3_conv1 = 16, f3_conv5 = 48, f4 = 64)\n",
        "\n",
        "  # Extra network 1:\n",
        "  X1 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n",
        "  X1 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X1)\n",
        "  X1 = Flatten()(X1)\n",
        "  X1 = Dense(1024, activation = 'relu')(X1)\n",
        "  X1 = Dropout(0.7)(X1)\n",
        "  X1 = Dense(5, activation = 'softmax')(X1)\n",
        "\n",
        "  \n",
        "  # 4th Inception block\n",
        "  X = Inception_block(X, f1 = 160, f2_conv1 = 112, f2_conv3 = 224, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)\n",
        "\n",
        "  # 5th Inception block\n",
        "  X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 256, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)\n",
        "\n",
        "  # 6th Inception block\n",
        "   X = Inception_block(X, f1 = 112, f2_conv1 = 144, f2_conv3 = 288, f3_conv1 = 32, f3_conv5 = 64, f4 = 64)\n",
        "\n",
        "  # Extra network 2:\n",
        "  X2 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n",
        "  X2 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X2)\n",
        "  X2 = Flatten()(X2)\n",
        "  X2 = Dense(1024, activation = 'relu')(X2)\n",
        "  X2 = Dropout(0.7)(X2)\n",
        "  X2 = Dense(1000, activation = 'softmax')(X2)\n",
        "  \n",
        "  \n",
        "  # 7th Inception block\n",
        "  X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, \n",
        "                      f3_conv5 = 128, f4 = 128)\n",
        "\n",
        "  # max-pooling layer: pool_size = (3,3), strides = 2\n",
        "  X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n",
        "\n",
        "  # 8th Inception block\n",
        "  X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, f3_conv5 = 128, f4 = 128)\n",
        "\n",
        "  # 9th Inception block\n",
        "  X = Inception_block(X, f1 = 384, f2_conv1 = 192, f2_conv3 = 384, f3_conv1 = 48, f3_conv5 = 128, f4 = 128)\n",
        "\n",
        "  # Global Average pooling layer \n",
        "  X = GlobalAveragePooling2D(name = 'GAPL')(X)\n",
        "\n",
        "  # Dropoutlayer \n",
        "  X = Dropout(0.4)(X)\n",
        "\n",
        "  # output layer \n",
        "   X = Dense(1000, activation = 'softmax')(X)\n",
        "  \n",
        "  # model\n",
        "  model = Model(input_layer, [X, X1, X2], name = 'GoogLeNet')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "LjyKxolC0ugN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}